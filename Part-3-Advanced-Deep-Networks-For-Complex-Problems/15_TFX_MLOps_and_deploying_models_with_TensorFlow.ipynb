{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Part3Header"
   },
   "source": [
    "**PART 3**\n",
    "\n",
    "**ADVANCED DEEP NETWORKS FOR COMPLEX PROBLEMS**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chap15Header"
   },
   "source": [
    "**CHAPTER 15 - TFX: MLOps and deploying models with TensorFlow**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_Intro"
   },
   "source": [
    "### **15.1 Writing a data pipeline with TFX**\n",
    "\n",
    "**MLOps** (Machine Learning Operations) combines ML and DevOps to automate the lifecycle of machine learning applications, from data collection to model delivery. **TFX (TensorFlow Extended)** is a powerful platform for building and managing these end-to-end machine learning pipelines.\n",
    "\n",
    "In this chapter, we develop a pipeline to predict the severity of forest fires based on weather conditions using the **Forest Fires** dataset. The pipeline involves several stages: data ingestion, validation, feature transformation, model training, and deployment.\n",
    "\n",
    "#### **15.1.1 Loading data from CSV files**\n",
    "\n",
    "The first step in any pipeline is data ingestion. TFX provides the `CsvExampleGen` component specifically for reading CSV files. This component splits the data into training and evaluation sets (using hashing) and converts them into **TFRecord** format (serialized byte streams), which is the standard efficient data format for TensorFlow.\n",
    "\n",
    "![Figure 15.1 The directory/file structure after running the CsvExampleGen](./15.Chapter-15/Figure15-1.jpg)\n",
    "\n",
    "After running, the component generates an output artifact containing the split data files (e.g., `train` and `eval` splits).\n",
    "\n",
    "![Figure 15.2 Output HTML table generated by running the CsvExampleGen component](./15.Chapter-15/Figure15-2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_CsvExampleGen"
   },
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "import os\n",
    "\n",
    "# Define the component to read data from the directory containing CSVs\n",
    "example_gen = CsvExampleGen(input_base=os.path.join('data', 'csv', 'train'))\n",
    "\n",
    "# Run the component within the interactive context\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_1_2"
   },
   "source": [
    "#### **15.1.2 Generating basic statistics from the data**\n",
    "\n",
    "Understanding the data through **Exploratory Data Analysis (EDA)** is critical. The `StatisticsGen` component iterates over the data produced by `CsvExampleGen` and computes descriptive statistics (e.g., mean, standard deviation, missing value counts, min/max) for both training and evaluation splits.\n",
    "\n",
    "![Figure 15.3 The output provided by the StatisticsGen component](./15.Chapter-15/Figure15-3.jpg)\n",
    "![Figure 15.4 The directory/file structure after running StatisticsGen](./15.Chapter-15/Figure15-4.jpg)\n",
    "\n",
    "Visualizing these statistics allows us to detect data skewness, missing values, or anomalies. For example, we might observe the distribution of the `FFMC` feature or categorical counts for `day` and `month`.\n",
    "\n",
    "![Figure 15.5 The summary statistics graphs generated for the data by the StatisticsGen component](./15.Chapter-15/Figure15-5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_StatisticsGen"
   },
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen\n",
    "\n",
    "# Generate statistics using the examples produced by CsvExampleGen\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples']\n",
    ")\n",
    "context.run(statistics_gen)\n",
    "\n",
    "# Visualize the statistics in the notebook\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_1_3"
   },
   "source": [
    "#### **15.1.3 Inferring the schema from data**\n",
    "\n",
    "The `SchemaGen` component automatically infers a data schema based on the statistics generated in the previous step. This schema acts as a blueprint for the data, defining:\n",
    "* **Data Types**: e.g., FLOAT, INT, STRING.\n",
    "* **Presence**: Whether a feature is required or optional.\n",
    "* **Domains**: The set of allowed values for categorical features or ranges for numerical ones.\n",
    "\n",
    "We set `infer_feature_shape=False` to provide flexibility for downstream feature engineering steps, which means the data will be represented as sparse tensors during the transformation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_SchemaGen"
   },
   "outputs": [],
   "source": [
    "from tfx.components import SchemaGen\n",
    "\n",
    "# Infer schema from statistics\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False\n",
    ")\n",
    "context.run(schema_gen)\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_1_4"
   },
   "source": [
    "#### **15.1.4 Converting data to features**\n",
    "\n",
    "The `Transform` component is responsible for feature engineering. It takes the raw data and the schema, and applies a user-defined preprocessing function (`preprocessing_fn`). This function is typically defined in a separate Python module file (e.g., `forest_fires_transform.py`).\n",
    "\n",
    "Common transformations include:\n",
    "* **Z-score Normalization**: For dense floating-point features (scaling to mean 0, variance 1).\n",
    "* **Vocabulary Generation**: Mapping strings to integer IDs for categorical features.\n",
    "* **Bucketization**: Grouping continuous numerical values into discrete bins (e.g., bucketizing Relative Humidity).\n",
    "\n",
    "The `preprocessing_fn` leverages the `tensorflow_transform` (tft) library to perform these operations efficiently and consistently during both training and serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Transform"
   },
   "outputs": [],
   "source": [
    "%%writefile forest_fires_transform.py\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import forest_fires_constants\n",
    "\n",
    "# ... (Import constants like feature keys) ...\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    outputs = {}\n",
    "    # Scale dense features using Z-score\n",
    "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.scale_to_z_score(\n",
    "            _sparse_to_dense(inputs[key])\n",
    "        )\n",
    "    # Apply vocabulary for categorical features\n",
    "    for key in _VOCAB_FEATURE_KEYS:\n",
    "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
    "            _sparse_to_dense(inputs[key]), \n",
    "            num_oov_buckets=1\n",
    "        )\n",
    "    # Bucketize numerical features based on defined boundaries\n",
    "    for key, boundary in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n",
    "        outputs[_transformed_name(key)] = tft.apply_buckets(\n",
    "            _sparse_to_dense(inputs[key]), bucket_boundaries=[boundary]\n",
    "        )\n",
    "    # Pass the label through unchanged\n",
    "    outputs[_transformed_name(_LABEL_KEY)] = _sparse_to_dense(inputs[_LABEL_KEY])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_RunTransform"
   },
   "outputs": [],
   "source": [
    "from tfx.components import Transform\n",
    "\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath('forest_fires_transform.py')\n",
    ")\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_2"
   },
   "source": [
    "### **15.2 Training a simple regression neural network: TFX Trainer API**\n",
    "\n",
    "The `Trainer` component is the core engine for model training in TFX. It consumes the transformed data and the transform graph produced by the previous step. We define a `run_fn` function (in a separate module) to orchestrate data loading, model building, and the training loop.\n",
    "\n",
    "![Figure 15.6 How the model interacts with the API, the TensorFlow server, and the client](./15.Chapter-15/Figure15-6.jpg)\n",
    "\n",
    "#### **15.2.1 Defining a Keras model**\n",
    "To handle the various feature types (dense, categorical, bucketized), we utilize `tf.feature_column` definitions. These columns are then fed into a `DenseFeatures` layer, which serves as the entry point to our neural network. The model architecture is a deep regressor consisting of several hidden layers and a single output node for the regression target.\n",
    "\n",
    "![Figure 15.7 Overview of the functionality of the DenseFeatures layer](./15.Chapter-15/Figure15-7.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_BuildModel"
   },
   "outputs": [],
   "source": [
    "def _build_keras_model(columns, dnn_hidden_units):\n",
    "    # ... (Define Input layers for all feature columns) ...\n",
    "    \n",
    "    # Use DenseFeatures layer to process inputs based on feature columns\n",
    "    output = tf.keras.layers.DenseFeatures(columns)(input_layers)\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for numnodes in dnn_hidden_units:\n",
    "        output = tf.keras.layers.Dense(numnodes, activation='tanh')(output)\n",
    "    \n",
    "    # Output layer for regression (1 unit)\n",
    "    output = tf.keras.layers.Dense(1)(output)\n",
    "    \n",
    "    model = tf.keras.Model(input_layers, output)\n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_2_2"
   },
   "source": [
    "#### **15.2.2 Defining the model training**\n",
    "The `run_fn` function ties everything together. It:\n",
    "1.  Loads the transformation graph.\n",
    "2.  Creates training and evaluation datasets using a custom `_input_fn`.\n",
    "3.  Builds the Keras model.\n",
    "4.  Trains the model using `model.fit()`.\n",
    "5.  Saves the trained model along with specific **signatures** that define how it should be served."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_RunFn"
   },
   "outputs": [],
   "source": [
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    # Load the transform output\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = _input_fn(fn_args.train_files, ...)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, ...)\n",
    "    \n",
    "    # Build the model\n",
    "    model = _build_keras_model(...)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_dataset, ...)\n",
    "    \n",
    "    # Define signatures for serving\n",
    "    signatures = {\n",
    "        'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(...)\n",
    "    }\n",
    "    # Save the model\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_2_3"
   },
   "source": [
    "#### **15.2.3 Signature Defs: Defining how models are used outside TensorFlow**\n",
    "Signatures are crucial for deployment. They act as the API contract for the model. We define a serving function `serve_tf_examples_fn` which:\n",
    "1.  Accepts serialized examples (raw byte data).\n",
    "2.  Parses them.\n",
    "3.  Applies the transformation logic (embedding the Transform layer directly into the serving graph).\n",
    "4.  Generates predictions.\n",
    "This approach ensures that the deployed model can handle raw data directly without needing a separate preprocessing service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_2_4"
   },
   "source": [
    "#### **15.2.4 Training the Keras model with TFX Trainer**\n",
    "We execute the training by instantiating the `Trainer` component. We pass it the module file containing our training logic, the transformed data, the schema, and the transform graph. The Trainer outputs the saved model artifacts to the pipeline root.\n",
    "\n",
    "![Figure 15.8 The complete directory/file structure after running the Trainer](./15.Chapter-15/Figure15-8.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Trainer"
   },
   "outputs": [],
   "source": [
    "from tfx.components import Trainer\n",
    "from tfx.proto import trainer_pb2\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file=os.path.abspath(\"forest_fires_trainer.py\"),\n",
    "    transformed_examples=transform.outputs['transformed_examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=n_train_steps),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=n_eval_steps)\n",
    ")\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_3"
   },
   "source": [
    "### **15.3 Setting up Docker to serve a trained model**\n",
    "\n",
    "To create a portable and isolated environment for our model, we use **Docker**. We pull the official `tensorflow/serving` Docker image, which is optimized for serving TensorFlow models via a REST API.\n",
    "\n",
    "We start a container mapping the model's directory from our host machine to a directory inside the container, and we expose port **8501** for REST API communication. The command typically looks like:\n",
    "`docker run --rm -p 8501:8501 ... tensorflow/serving:2.6.3-gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec15_4"
   },
   "source": [
    "### **15.4 Deploying the model and serving it through an API**\n",
    "\n",
    "Before a model goes live, it must pass validation checks. TFX provides specialized components for this:\n",
    "\n",
    "#### **15.4.1 Validating the infrastructure**\n",
    "The `InfraValidator` component verifies that the model is mechanically serveable. It spins up a test Docker container, loads the model, and sends test requests. If the model responds correctly, it is marked as \"BLESSED\" (infrastructure-wise).\n",
    "\n",
    "![Figure 15.9 The directory/file structure after running the InfraValidator](./15.Chapter-15/Figure15-9.jpg)\n",
    "\n",
    "#### **15.4.2 Resolving the correct model**\n",
    "The `Resolver` component is used to manage model versions. Using a strategy like `LatestBlessedModelStrategy`, it identifies the best previously trained model (baseline) to compare the new model against.\n",
    "\n",
    "#### **15.4.3 Evaluating the model**\n",
    "The `Evaluator` component performs deep analysis of model performance. It computes metrics (e.g., Mean Squared Error) on the evaluation set and checks them against defined thresholds. It also compares the current model to the baseline. Only if the new model meets the thresholds and outperforms the baseline is it \"blessed\" for quality.\n",
    "\n",
    "#### **15.4.4 Pushing the final model**\n",
    "The `Pusher` component is the final gatekeeper. It pushes the model to the production destination (e.g., a serving directory) only if the model has been blessed by both the `InfraValidator` and the `Evaluator`.\n",
    "\n",
    "#### **15.4.5 Predicting with the TensorFlow serving API**\n",
    "Once the model is served (e.g., via Docker), clients can send HTTP POST requests to the API endpoint (e.g., `http://localhost:8501/v1/models/forest_fires_model:predict`). The request body typically contains the input data, often encoded in base64 if it involves binary formats or serialized examples.\n",
    "\n",
    "![Figure 15.10 How the model interacts with the API, the TensorFlow server, and the client](./15.Chapter-15/Figure15-10.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Predict"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "# Construct the request body\n",
    "# Note: Input data is often base64 encoded for transmission\n",
    "req_body = {\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": [\n",
    "        str(base64.b64encode(b\"{\\\"X\\\": 7, ...}\")) # Example input data\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send HTTP POST request to the running TensorFlow Serving container\n",
    "json_response = requests.post(\n",
    "    'http://localhost:8501/v1/models/forest_fires_model:predict',\n",
    "    data=json.dumps(req_body),\n",
    "    headers={\"content-type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "# Parse and print predictions\n",
    "predictions = json.loads(json_response.text)\n",
    "print(predictions)"
   ]
  }
 ]
}