{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Part3Header"
   },
   "source": [
    "**PART 3**\n",
    "\n",
    "**ADVANCED DEEP NETWORKS FOR COMPLEX PROBLEMS**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chap12Header"
   },
   "source": [
    "**CHAPTER 12 - Sequence-to-sequence learning: Part 2**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec12_Intro"
   },
   "source": [
    "In the previous chapter, we built an English-to-German machine translator using a standard Encoder-Decoder architecture. We used teacher forcing for training and evaluated it using the BLEU score. Finally, we repurposed the model for inference using a recursive decoder.\n",
    "\n",
    "In this chapter, we improve the model's accuracy by implementing the **attention mechanism**. This allows the decoder to access rich representations from all time steps of the input sequence, rather than relying solely on the final context vector. We will also visualize these attention mechanisms to gain insights into the model's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec12_1"
   },
   "source": [
    "### **12.1 Eyeballing the past: Improving our model with attention**\n",
    "\n",
    "Standard seq2seq models suffer from a bottleneck: the encoder must compress the entire input sequence into a single fixed-size context vector. This is often insufficient for long sequences.\n",
    "\n",
    "**Bahdanau Attention** solves this by allowing the decoder to look at *all* the encoder's outputs at every time step. For each decoding step, the model computes a weighted sum of the encoder outputs (the context vector), where the weights represent the importance of each input word to the current output word.\n",
    "\n",
    "The process involves:\n",
    "1.  **Energy Computation**: Calculating \"energy\" scores using a small fully connected network ($W, U, v$) that measures the match between the previous decoder state and each encoder output.\n",
    "2.  **Normalization**: Using Softmax to convert energies into probabilities (attention weights, $\\alpha$).\n",
    "3.  **Context Vector**: Computing the weighted sum of encoder outputs using these probabilities.\n",
    "\n",
    "#### **12.1.1 Implementing Bahdanau attention in TensorFlow**\n",
    "\n",
    "We implement a custom layer `DecoderRNNAttentionWrapper` using Keras subclassing because TensorFlow does not provide a built-in layer for this specific architecture.\n",
    "\n",
    "* **`__init__`**: Initializes the layer with a standard RNN cell (e.g., `GRUCell`).\n",
    "* **`build`**: Defines the trainable weight matrices $W_a$, $U_a$, and $V_a$.\n",
    "* **`call`**: Uses `K.rnn` to iterate through the decoder inputs. It defines a `_step` function that calculates the attention energies, weights, and the final context vector for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_AttentionWrapper"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "class DecoderRNNAttentionWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self, cell_fn, units, **kwargs):\n",
    "        self.cell_fn = cell_fn\n",
    "        self.units = units\n",
    "        super(DecoderRNNAttentionWrapper, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "            shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "            initializer='uniform', trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "            shape=tf.TensorShape((self.cell_fn.units, self.cell_fn.units)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "            shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "            initializer='uniform', trainable=True)\n",
    "        super(DecoderRNNAttentionWrapper, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, initial_state, training=False):\n",
    "        encoder_outputs, decoder_inputs = inputs\n",
    "        \n",
    "        def _step(inputs, states):\n",
    "            # Step function for computing energy for a single decoder state\n",
    "            encoder_full_seq = states[-1]\n",
    "            \n",
    "            # Compute energy scores\n",
    "            W_a_dot_h = K.dot(encoder_outputs, self.W_a)\n",
    "            U_a_dot_s = K.expand_dims(K.dot(states[0], self.U_a), 1)\n",
    "            Wh_plus_Us = K.tanh(W_a_dot_h + U_a_dot_s)\n",
    "            \n",
    "            # Calculate attention weights (alpha)\n",
    "            e_i = K.squeeze(K.dot(Wh_plus_Us, self.V_a), axis=-1)\n",
    "            a_i = K.softmax(e_i)\n",
    "            \n",
    "            # Compute weighted sum (context vector)\n",
    "            c_i = K.sum(encoder_outputs * K.expand_dims(a_i, -1), axis=1)\n",
    "            \n",
    "            # Concatenate input and context vector, then pass to GRU cell\n",
    "            s, states = self.cell_fn(K.concatenate([inputs, c_i], axis=-1), states)\n",
    "            return (s, a_i), states\n",
    "\n",
    "        attn_outputs, _ = K.rnn(\n",
    "            step_function=_step, inputs=decoder_inputs,\n",
    "            initial_states=[initial_state], constants=[encoder_outputs]\n",
    "        )\n",
    "        \n",
    "        attn_out, attn_energy = attn_outputs\n",
    "        return attn_out, attn_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec12_1_2"
   },
   "source": [
    "#### **12.1.2 Defining the final model**\n",
    "\n",
    "We integrate the attention wrapper into the final seq2seq model.\n",
    "\n",
    "* **Encoder**: Remains a Bidirectional GRU.\n",
    "* **Decoder**: Now uses the `DecoderRNNAttentionWrapper` wrapping a `GRUCell`. Unlike the standard decoder, it takes **all** encoder states (`en_states`) as input to calculate attention scores.\n",
    "\n",
    "We also define helper functions `get_vectorizer` and `get_encoder` similar to the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_FinalModelWithAttention"
   },
   "outputs": [],
   "source": [
    "def get_final_seq2seq_model_with_attention(n_vocab, encoder, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model with attention \"\"\"\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
    "    fwd_state, bwd_state, en_states = encoder(e_inp)\n",
    "    \n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    d_vectorized_out = vectorizer(d_inp)\n",
    "    \n",
    "    d_emb_layer = tf.keras.layers.Embedding(\n",
    "        n_vocab+2, 128, mask_zero=True, name='d_embedding'\n",
    "    )\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    \n",
    "    d_init_state = tf.keras.layers.Concatenate(axis=-1)([fwd_state, bwd_state])\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    gru_cell = tf.keras.layers.GRUCell(256)\n",
    "    attn_out, _ = DecoderRNNAttentionWrapper(\n",
    "        cell_fn=gru_cell, units=512, name=\"d_attention\"\n",
    "    )([en_states, d_emb_out], initial_state=d_init_state)\n",
    "    \n",
    "    d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    d_densel_out = d_dense_layer_1(attn_out)\n",
    "    \n",
    "    d_final_layer = tf.keras.layers.Dense(\n",
    "        n_vocab+2, activation='softmax', name='d_dense_final'\n",
    "    )\n",
    "    d_final_out = d_final_layer(d_densel_out)\n",
    "    \n",
    "    seq2seq = tf.keras.models.Model(\n",
    "        inputs=[e_inp, d_inp], outputs=d_final_out, \n",
    "        name='final_seq2seq_with_attention'\n",
    "    )\n",
    "    return seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec12_1_3"
   },
   "source": [
    "#### **12.1.3 Training the model**\n",
    "\n",
    "The training process is identical to the one used for the standard seq2seq model. We use the custom training loop `train_model` (defined in Chapter 11) which employs teacher forcing and evaluates the model using the BLEU score.\n",
    "\n",
    "The attention-based model typically achieves significantly higher performance (e.g., doubling the BLEU score) compared to the standard model, demonstrating the power of the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Training"
   },
   "outputs": [],
   "source": [
    "# Example training call (assuming data and helper functions are prepared)\n",
    "# epochs = 5\n",
    "# batch_size = 128\n",
    "# train_model(final_model_with_attention, de_vectorizer, train_df, valid_df, \n",
    "#             test_df, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec12_2"
   },
   "source": [
    "### **12.2 Visualizing the attention**\n",
    "\n",
    "One of the major advantages of attention is **interpretability**. The attention weights (energies) tell us exactly which words in the source sentence the model focused on when generating a specific word in the target sentence.\n",
    "\n",
    "To visualize this, we define a special `attention_visualizer` model that returns the attention states (`attn_states`) alongside the standard predictions. We then use `matplotlib` to plot these weights as a heatmap.\n",
    "\n",
    "![Figure 12.1 Attention patterns visualized for an input English text](./12.Chapter-12/Figure12-1.jpg)\n",
    "\n",
    "A strong diagonal pattern in the heatmap usually indicates that the model has learned the correct alignment between the languages (e.g., the first English word corresponds to the first German word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Visualization"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def attention_visualizer(save_path):\n",
    "    \"\"\" Define the attention visualizer model \"\"\"\n",
    "    model = tf.keras.models.load_model(save_path)\n",
    "    # ... (Code to retrace layers and capture attention states) ...\n",
    "    # Returns a model that outputs [d_final_out, attn_states, e_vec_out, d_vec_out]\n",
    "    return visualizer_model\n",
    "\n",
    "def visualize_attention(visualizer_model, en_vocabulary, de_vocabulary, \n",
    "                        sample_en_text, sample_de_text, fig_savepath):\n",
    "    \"\"\" Visualize the attention patterns \"\"\"\n",
    "    d_pred, attention_weights, e_out, d_out = visualizer_model.predict(\n",
    "        [np.array([sample_en_text]), np.array([sample_de_text])]\n",
    "    )\n",
    "    \n",
    "    # Filtering and Plotting logic\n",
    "    fig, ax = plt.subplots(figsize=(14, 14))\n",
    "    im = ax.imshow(attention_weights_filtered)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticklabels(x_ticklabels)\n",
    "    ax.set_yticklabels(y_ticklabels)\n",
    "    \n",
    "    plt.colorbar(im)\n",
    "    plt.savefig(fig_savepath)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}