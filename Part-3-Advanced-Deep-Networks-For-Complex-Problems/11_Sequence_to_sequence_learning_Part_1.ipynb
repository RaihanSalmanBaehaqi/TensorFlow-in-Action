{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Part3Header"
   },
   "source": [
    "**PART 3**\n",
    "\n",
    "**ADVANCED DEEP NETWORKS FOR COMPLEX PROBLEMS**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Chap11Header"
   },
   "source": [
    "**CHAPTER 11 - Sequence-to-sequence learning: Part 1**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec11_1"
   },
   "source": [
    "### **11.1 Understanding the machine translation data**\n",
    "\n",
    "In this chapter, we explore **sequence-to-sequence (seq2seq)** learning, a paradigm used for tasks where an arbitrary-length input sequence is mapped to an arbitrary-length output sequence. Our specific goal is to build an **English-to-German machine translator**.\n",
    "\n",
    "**Data Acquisition and Cleaning**\n",
    "\n",
    "We utilize a bilingual parallel corpus provided by *manythings.org*, which contains English sentences paired with their German translations. The data requires rigorous preprocessing to ensure the model can learn effectively:\n",
    "\n",
    "1.  **Cleaning**: The raw data often contains metadata (e.g., attribution info like \"CC-BY 2.0\") that is irrelevant to the translation task and must be stripped. We also filter out lines with problematic Unicode characters (such as `\\xc2`) that can cause encoding errors in downstream TensorFlow operations.\n",
    "2.  **Special Tokens**: To enable the model to generate text efficiently, we explicitly add two special tokens to the German translations:\n",
    "    * `sos` (Start of Sentence): Marks the beginning of the translation.\n",
    "    * `eos` (End of Sentence): Marks the end of the translation.\n",
    "    These tokens serve as functional signals for the decoder during inference, telling it when to start generating text and when to stop.\n",
    "3.  **Sampling and Splitting**: To ensure the training completes in a reasonable time for this exercise, we sample a subset of 50,000 phrase pairs. These are strictly split into training (80%), validation (10%), and testing (10%) sets to monitor overfitting and evaluate generalization.\n",
    "\n",
    "**Vocabulary Analysis**\n",
    "\n",
    "Before modeling, we must understand the data distribution. We analyze the **vocabulary size** (number of unique words) and **sequence length** (number of words per sentence). This analysis helps us define the input dimension for our embedding layers and the padding length for our batches. The following code calculates the vocabulary size based on word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_VocabSize"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
    "    counter = Counter(words)\n",
    "    freq_df = pd.Series(\n",
    "        list(counter.values()),\n",
    "        index=list(counter.keys())\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(freq_df.head(n=10))\n",
    "        \n",
    "    n_vocab = (freq_df >= n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>= {} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec11_2_Intro"
   },
   "source": [
    "### **11.2 Writing an English-German seq2seq machine translator**\n",
    "\n",
    "The core architecture for this task is the **Encoder-Decoder** model. This architecture decouples the understanding of the input from the generation of the output, allowing the model to handle sequences of different lengths (e.g., a 5-word English sentence translating to a 7-word German sentence).\n",
    "\n",
    "1.  **Encoder**: Consumes the source language (English) and compresses the information into a latent representation called a **context vector** (or thought vector).\n",
    "2.  **Decoder**: Takes the context vector and generates the target language (German).\n",
    "\n",
    "![Figure 11.1 High-level components of the encoder-decoder architecture in the context of machine translation](./11.Chapter-11/Figure11-1.jpg)\n",
    "\n",
    "#### **The TextVectorization layer**\n",
    "\n",
    "To build a modern, end-to-end model, we integrate preprocessing directly into the network using the `TextVectorization` layer. This eliminates the need for external tokenization scripts and ensures the model can accept raw strings as input during deployment.\n",
    "\n",
    "* **Functionality**: It tokenizes strings (splitting them into words) and maps them to integer IDs based on a learned vocabulary lookup.\n",
    "* **Configuration**:\n",
    "    * `output_mode='int'`: Ensures the layer outputs integer indices suitable for an Embedding layer.\n",
    "    * `adapt()`: This method scans the training corpus to build the internal vocabulary mapping from words to integers.\n",
    "\n",
    "#### **Defining the TextVectorization layers for the seq2seq model**\n",
    "\n",
    "We define a helper function, `get_vectorizer`, to instantiate these layers for both the source (English) and target (German) languages. Note that we set the vocabulary size to `n_vocab + 2` to explicitly account for the padding token (used to make batches uniform) and the `[UNK]` token (used for out-of-vocabulary words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Vectorizer"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,\n",
    "    )\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    if not return_vocabulary:\n",
    "        return tf.keras.models.Model(\n",
    "            inputs=inp, outputs=vectorized_out, name=name\n",
    "        )\n",
    "    else:\n",
    "        return tf.keras.models.Model(\n",
    "            inputs=inp, outputs=vectorized_out, name=name\n",
    "        ), vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec11_2_Encoder"
   },
   "source": [
    "#### **Defining the encoder**\n",
    "\n",
    "The encoder's primary role is to ingest the source English sequence and compress its semantic meaning into a latent representation known as the **context vector**.\n",
    "\n",
    "* **Bidirectional Processing**: We use a `Bidirectional` wrapper around a GRU (Gated Recurrent Unit) layer. Unlike a standard RNN that reads strictly left-to-right, a bidirectional RNN processes the sequence in both directions. This is critical for capturing dependencies where the meaning of a word is influenced by future words (e.g., distinguishing the word \"bank\" in \"bank of the river\" vs. \"bank of America\").\n",
    "* **Context Vector Extraction**: While the GRU processes the entire sequence, we are specifically interested in the **final state** (not the sequence of outputs). This final state represents the aggregated understanding of the entire sentence and is passed to the decoder as its initial state.\n",
    "\n",
    "![Figure 11.2 Specific components in the encoder and decoder modules](./11.Chapter-11/Figure11-2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Encoder"
   },
   "outputs": [],
   "source": [
    "def get_encoder(n_vocab, vectorizer):\n",
    "    \"\"\" Define the encoder of the seq2seq model \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Embedding layer with mask_zero=True to handle padding\n",
    "    emb_layer = tf.keras.layers.Embedding(\n",
    "        n_vocab+2, 128, mask_zero=True, name='e_embedding'\n",
    "    )\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    gru_layer = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, name='e_gru'),\n",
    "        name='e_bidirectional_gru'\n",
    "    )\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    # The encoder returns the final state (context vector)\n",
    "    encoder = tf.keras.models.Model(\n",
    "        inputs=inp, outputs=gru_out, name='encoder'\n",
    "    )\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec11_2_Decoder"
   },
   "source": [
    "#### **Defining the decoder and the final model**\n",
    "\n",
    "The decoder is responsible for generating the German translation, token by token. It differs from the encoder in several key ways:\n",
    "\n",
    "* **Initialization**: The decoder's GRU layer does not start with a zero state. Instead, its `initial_state` is set to the encoder's output (`d_init_state`). This transfers the knowledge from the English sentence to the German generation process.\n",
    "* **Unidirectional**: The decoder is a standard, unidirectional GRU because it generates text sequentially and cannot \"see\" the future words it hasn't generated yet.\n",
    "* **Teacher Forcing**: During training, we employ **Teacher Forcing**, where the decoder is fed the *correct* German word at time $t$ to predict the word at $t+1$. This stabilizes training and helps the model converge faster than if it relied solely on its own predictions.\n",
    "* **Sequence Output**: The decoder sets `return_sequences=True` because it must output a prediction for every time step in the target sequence, which is then passed through a Softmax layer to predict the specific word from the vocabulary.\n",
    "\n",
    "![Figure 11.3 The implementation of the final sequence-to-sequence model with the focus on various layers and outputs involved](./11.Chapter-11/Figure11-3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_FinalModel"
   },
   "outputs": [],
   "source": [
    "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
    "    # 1. Get Context Vector from Encoder\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
    "    d_init_state = encoder(e_inp)\n",
    "    \n",
    "    # 2. Define Decoder Input (Teacher Forcing Input)\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    d_vectorized_out = vectorizer(d_inp)\n",
    "    \n",
    "    d_emb_layer = tf.keras.layers.Embedding(\n",
    "        n_vocab+2, 128, mask_zero=True, name='d_embedding'\n",
    "    )\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    \n",
    "    # 3. Decoder GRU initialized with Encoder State\n",
    "    d_gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
    "    \n",
    "    # 4. Final Prediction Layers\n",
    "    d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    d_densel_out = d_dense_layer_1(d_gru_out)\n",
    "    \n",
    "    d_dense_layer_final = tf.keras.layers.Dense(\n",
    "        n_vocab+2, activation='softmax', name='d_dense_final'\n",
    "    )\n",
    "    d_final_out = d_dense_layer_final(d_densel_out)\n",
    "    \n",
    "    seq2seq = tf.keras.models.Model(\n",
    "        inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq'\n",
    "    )\n",
    "    return seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec11_2_Compile"
   },
   "source": [
    "#### **Compiling the model**\n",
    "\n",
    "We compile the model using standard Keras settings for multi-class classification (predicting the next word from the vocabulary).\n",
    "\n",
    "* **Loss**: `sparse_categorical_crossentropy` (targets are integers, predictions are probabilities).\n",
    "* **Optimizer**: `adam`.\n",
    "* **Metrics**: `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Compile"
   },
   "outputs": [],
   "source": [
    "# Example compilation code (assuming model instance 'final_model' exists)\n",
    "# final_model.compile(\n",
    "#    loss='sparse_categorical_crossentropy',\n",
    "#    optimizer='adam',\n",
    "#    metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec11_3"
   },
   "source": [
    "### **11.3 Training and evaluating the model**\n",
    "\n",
    "**Data Preparation for Teacher Forcing**\n",
    "Training a machine translation model requires careful alignment of inputs and outputs to support the Teacher Forcing mechanism. We cannot simply feed the entire German sentence as both input and label; we must offset them by one time step.\n",
    "\n",
    "* **Encoder Input**: The source English text (e.g., \"I like cake\").\n",
    "* **Decoder Input**: The target German text *including* the `sos` token but *excluding* the last token (e.g., \"sos Ich mag Kuchen\").\n",
    "* **Decoder Label**: The target German text *excluding* the `sos` token but *including* the `eos` token (e.g., \"Ich mag Kuchen eos\").\n",
    "\n",
    "This setup forces the model to predict the next token (`Ich`) given the current token (`sos`).\n",
    "\n",
    "**Evaluating with BLEU**\n",
    "\n",
    "Standard classification accuracy is a poor metric for machine translation because there are often multiple valid ways to translate a sentence. To evaluate our model effectively, we implement the **BLEU (BiLingual Evaluation Understudy)** score.\n",
    "\n",
    "* **Mechanism**: BLEU measures the overlap of n-grams (sequences of 1, 2, 3, or more words) between the model's prediction and the reference translation, providing a more nuanced view of quality than simple word-matching.\n",
    "* **Brevity Penalty**: The metric includes a penalty for translations that are significantly shorter than the reference, preventing the model from \"gaming\" the score by outputting very short, high-confidence snippets.\n",
    "\n",
    "Since BLEU is not a built-in Keras metric, we implement a custom `BLEUMetric` class and a custom training loop to compute it during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_BLEU"
   },
   "outputs": [],
   "source": [
    "class BLEUMetric(object):\n",
    "    def __init__(self, vocabulary, name='bleu', **kwargs):\n",
    "        self.vocab = vocabulary\n",
    "        # StringLookup to convert IDs back to words for comparison\n",
    "        self.id_to_token_layer = tf.keras.layers.StringLookup(\n",
    "            vocabulary=self.vocab, invert=True, num_oov_indices=0\n",
    "        )\n",
    "    \n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\" Decodes predictions and calculates BLEU score \"\"\"\n",
    "        # Convert probability distribution to token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        # (Detailed logic to clean text, remove padding/EOS/SOS, and call compute_bleu would exist here)\n",
    "        # This step is crucial for accurate BLEU calculation\n",
    "        \n",
    "        return 0.0 # Placeholder for result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sec11_4"
   },
   "source": [
    "### **11.4 From training to inference: Defining the inference model**\n",
    "\n",
    "**The Inference Challenge**\n",
    "\n",
    "The training model relies on Teacher Forcing, meaning it requires the target German sequence as input. However, in a real-world inference scenario, we do not have the German translationâ€”generating it is the goal!.\n",
    "\n",
    "**The Recursive Decoder Solution**\n",
    "\n",
    "To solve this, we construct a separate **Recursive Inference Model** that reuses the trained weights but alters the data flow to a recursive loop:\n",
    "\n",
    "1.  **Encoder Step**: Pass the English text through the encoder to generate the **context vector**. This vector is calculated once and serves as the seed for the decoder.\n",
    "2.  **Initialization**: We feed the `sos` (Start of Sentence) token and the context vector into the decoder to predict the *first* word.\n",
    "3.  **Recursive Loop**: For every subsequent step, we take the *predicted* word from the previous step (along with the decoder's updated internal state) and feed them back as inputs to predict the *next* word.\n",
    "4.  **Termination**: This loop continues until the decoder generates the `eos` (End of Sentence) token, signaling that the translation is complete.\n",
    "\n",
    "![Figure 11.4 Using the sequence-to-sequence model for inference (i.e., generating translations from English inputs)](./11.Chapter-11/Figure11-4.jpg)\n",
    "\n",
    "The code below demonstrates this recursive generation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Code_Inference"
   },
   "outputs": [],
   "source": [
    "def generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text):\n",
    "    \"\"\" Generate a new translation using recursive decoding \"\"\"\n",
    "    start_token = 'sos'\n",
    "    end_token = 'eos'\n",
    "    \n",
    "    # Step 1: Get the context vector from the encoder\n",
    "    d_state = en_model.predict(np.array([sample_en_text]))\n",
    "    \n",
    "    de_word = start_token\n",
    "    de_translation = []\n",
    "    \n",
    "    # Step 2 & 3: Recursive prediction loop\n",
    "    while de_word != end_token:\n",
    "        # Predict the next word and the NEW state given the previous word and state\n",
    "        # Note: de_model for inference is modified to accept state inputs\n",
    "        de_pred, d_state = de_model.predict([np.array([de_word]), d_state])\n",
    "        \n",
    "        # Convert the prediction index back to a string word\n",
    "        de_word = de_vocabulary[np.argmax(de_pred[0])]\n",
    "        de_translation.append(de_word)\n",
    "        \n",
    "    print(\"Translation: {}\".format(' '.join(de_translation)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}